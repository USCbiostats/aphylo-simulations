---
title: "Accuracy of aphylo"
author: "George G Vega Yon"
date: "March 14, 2019"
output:
  - pdf_document
  - html_document
section-titles: false
fontsize: 8pt
handout: false
page-number: true
classoption: aspectratio=169
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Recap

Last time:

1.  We ran a leave-one-out cross-validation algorithm in about 1,000 function-tree pairs

2.  Implemented function to calculate AUCs for our data (the AUC R package wasn't the best option for us)

3.  Will look at best and worse cases considering more than one annotation of 0/1 each.

4.  This will include reporting average shortest path lengths between pairs (more later).

Today:

1.  Look at our Prediction Score function (AUCs actually are not very good when predictions are around 50%)

2.  Look at individual predictions (and functions/trees for fun).


```{r data-prep, include=FALSE, cached=FALSE}
library(aphylo)
library(dplyr)
library(magrittr)

cross_validation_tab <- readRDS("cross_validation_tab.rds")
```

```{r top10, cache=FALSE}
# Tagging best 4 and worst 4 based on our measurement of pscore
top_10_cases <- cross_validation_tab %>%
  filter(n_used >= 4) %>%
  mutate(
    change = abs(pscore_obs - pscore_rand),
    bad    = pscore_obs > pscore_rand
  ) %>%
  group_by(bad) %>%
  arrange(desc(change)) %>%
  mutate(ordid = 1:n()) %>%
  filter(ordid <= 10) %>%
  select(-change, -ordid, -balance)
```

```{r geodesics, cache = TRUE, eval=FALSE}
# This function calculates geodesic distances (shortest paths)
gstat <- function(x) {
  D <- approx_geodesic(x$tree$edge, undirected = TRUE, nsteps = 50)
  diag(D) <- NA
  
  # Between 0-1
  ids0 <- which(x$tip.annotation == 0L)
  ids1 <- which(x$tip.annotation == 1L)
  
  tibble(
    zero2one  = mean(1/D[ids0, ids1], na.rm=TRUE),
    one2one   = mean(1/D[ids1, ids1], na.rm=TRUE),
    zero2zero = mean(1/D[ids0, ids0], na.rm=TRUE)
  )
  
}
```


## Overall fitness

Prediction score

\newcommand{\isone}[1]{{\boldsymbol{1}\left( #1 \right)}}
\renewcommand{\Pr}[1]{{\mathbb{P}\left(#1\right) }}
\newcommand{\f}[1]{{f\left(#1\right) }}
\newcommand{\Prcond}[2]{{\mathbb{P}\left(#1\vphantom{#2}\;\right|\left.\vphantom{#1}#2\right)}}
\newcommand{\fcond}[2]{{f\left(#1|#2\right) }}
\newcommand{\Expected}[1]{{\mathbb{E}\left\{#1\right\}}}
\newcommand{\ExpectedCond}[2]{{\mathbb{E}\left\{#1\vphantom{#2}\right|\left.\vphantom{#1}#2\right\}}}

\newcommand{\Likelihood}[2]{\text{L}\left(#1 \left|\vphantom{#1}#2\right.\right)}

\newcommand{\Ann}{\mathbf{X}{}} 
\newcommand{\ann}[1]{x_{#1}{}} 
\newcommand{\constraints}{\mathcal{C}{}} 
\newcommand{\AnnObs}{\mathbf{Z}{}}
\newcommand{\annObs}[1]{z_{#1}{}}
\newcommand{\AnnPred}{\hat X{}}
\newcommand{\annPred}[1]{\hat x_{#1}}


To measure the performance of our model we calculated a prediction score which compares truth (observed) versus predicted annotations in a probabilistic way. The details about its derivation and the reason why we did not used the popular Area Under the Curve statistic are provided in \autoref{sec:prediction-score}. In particular, using the observed annotations, we compute the following statistic:

\begin{equation*}
\delta = \sum_{n: \annObs{n} \in \{0, 1\}}(\annObs{n} - \annPred{n})^2
\end{equation*}

Which is a special case of the statistic described in the appendix when we assume no correlation across leafs, and a single function. Moreover, assuming that $\annPred{n}\sim \mbox{Bernoulli}(\alpha)$, the statistic has an expectation equal to:

\begin{equation*}
\ExpectedCond{\sum_{n: \annObs{n} \in \{0, 1\}}(\annObs{n} - \annPred{n})^2}{\AnnObs} = \sum_{n: \annObs{n} \in \{0, 1\}}\ExpectedCond{(\annObs{n} - \annPred{n})^2}{\AnnObs} = \sum_{n: \annObs{n} \in \{0, 1\}}(\annObs{n} - \alpha)^2
\end{equation*}

Finally, in order to improve interpretability, we normalize this statistic by the value it acquires in the worse case scenario, this is, when $\annPred{n} = 1 - \annObs{n}$, then its value equals

\begin{equation*}
	\sum_{n: \annObs{n} \in \{0, 1\}}(\annObs{n} - (1 - \annObs{n}))^2 = \sum_{n: \annObs{n} \in \{0, 1\}} 1 = |{n: \annObs{n} \in \{0, 1\}}|
\end{equation*}

Which we denote as $\underline{\delta}$. The normalized $\delta$ ranges from 0, perfect prediction, to 1, complete miss match.

```{r pscore-fig, fig.align='center', cache=TRUE}
library(ggplot2)

cross_validation_tab %>%
  filter(`0` > 0, `1` > 0) %>%
  mutate(ten_or_less = n_used <= 10) %>%
  ggplot(aes(
  x     = pscore_rand, 
  y     = pscore_obs,
  size  = n_used,
  color = ten_or_less
  )) + 
  geom_jitter(width = 0.01, height = 0) +
  geom_abline(slope=1, intercept = 0) + 
  xlab("Random") + ylab("Observed") + 
  labs(size = "N. of annotations used") +
  labs(color = "10 or less annotations") +
  annotate("text", x = .1, y =.6, label = "Bad") +
  annotate("text", x = .6, y =.1, label = "Good")


```

## What characterizes a better performance?

```{r betterperformance, results='asis'}
model0a <- cross_validation_tab %>%
  mutate(
    better = pscore_rand > pscore_obs,
    size   = `0` + `1` + `9`
    ) %>%
  glm(pscore_obs ~ `0` + `1` + `annotated` + prop0s + size + `0`*`size` + 
        `1`*`size`, data = ., family = gaussian()) 

model0b <- cross_validation_tab %>%
  mutate(
    better = pscore_rand > pscore_obs,
    size   = `0` + `1` + `9`
    ) %>%
  glm(pscore_rand ~ `0` + `1` + `annotated` + prop0s + size + `0`*`size` + 
        `1`*`size`, data = ., family = gaussian()) 

model1 <- cross_validation_tab %>%
  mutate(
    better = pscore_rand > pscore_obs,
    size   = `0` + `1` + `9`
    ) %>%
  glm(better ~ `0` + `1` + `annotated` + prop0s + size + `0`*`size` +  
        `1`*`size`, data = ., family = binomial()) 

model2 <- cross_validation_tab %>%
  mutate(
    better = pscore_obs - pscore_rand,
    size   = `0` + `1` + `9`
    ) %>%
  glm(better ~ `0` + `1` + `annotated` + prop0s + size + `0`*`size` + 
        `1`*`size`, data = ., family = gaussian()) 

# Adding the names to be printed pretty with texreg
model <- list(
  `Obs`             = model0a,
  Rand              = model0b,
  `1(Obs < Random)` = model1,
  `Rand - Obs`      = model2
  )

if (knitr::is_html_output()) {
  texreg::htmlreg(model, doctype  = FALSE)
} else {
  texreg::texreg(model)
}
```


## Trees with the most annotated leafs

Average inverse (undirected) shortest path length

\begin{equation}
S = |J|^{-1}\sum_{(i,j) \in J}g(i, j)^{-1}
\end{equation}

Where $g(i,j)$ is the shortest path length between the pair $(i,j)$.

We calculated this between 0-0 (zero2zero), 0-1 (zero2one), and 1-1 (one2one).

## Trees with the most annotated leafs (cont.)

```{r tables, cache=FALSE}
tab <- top_10_cases %>%
  select(-prop0s, -n_used, -aphylo, -prop0s_tag, -ann_expected, -ann_predicted)

if (knitr::is_html_output()) {
  tab$term <- sprintf("<a href=\"http://amigo.geneontology.org/amigo/term/%s\" target=\"_blank\">%1$s</a>", tab$term)
  tab$tree <- sprintf("<a href=\"http://pantherdb.org/panther/family.do?clsAccession=%s\" target=\"_blank\">%1$s</a>", tab$tree)
} else if (knitr::is_latex_output()) {
  tab$term <- sprintf("\\href{http://amigo.geneontology.org/amigo/term/%s}{%1$s}", tab$term)
  tab$tree <- sprintf("\\href{http://pantherdb.org/panther/family.do?clsAccession=%s}{%1$s}", tab$tree)
}
  

tab %>%
  knitr::kable(digits = 2, caption = "Best and worst prediction cases. The data included here has at least 4 annotations. A case is classified as bad if the random prediction score was lower than the observed prediction score.")

saveRDS(trees, "trees.rds")
saveRDS(top_10_cases, "top_10_cases.rds")
```


```{r fun-to-print-table-probabilities}

plot_tree <- function(i) {
  plot(
  top_10_cases$aphylo[[i]],
  show.node.label = FALSE,
  show.tip.label = FALSE,
  main = sprintf(
    "Tree ID: %s, %s\nAUC: %03.2f, PredScore: %03.2f, RandScore: %03.2f",
    top_10_cases$tree[i],
    colnames(top_10_cases$aphylo[[i]]$tip.annotation),
    top_10_cases$auc[i], top_10_cases$pscore_obs[i], 
    top_10_cases$pscore_rand[i]
    ), rect.args = list(border="transparent")
  )
}

table_probs <- function(i) {
  tibble(
      Expected = top_10_cases$ann_expected[[1]][,1],
      Predicted = top_10_cases$ann_predicted[[1]][,1]
      ) %>% arrange(Predicted) %>%
    knitr::kable(
      caption = sprintf(
        "Expected and Predicted Probabilities\nTree: %s, Function: %s",
        top_10_cases$tree[i], 
        top_10_cases$term[i]
      ))
}

```


## Best 4 (1/4)

```{r best4-1, cache = FALSE}
plot_tree(1)
table_probs(1)
```

## Best 4 (2/4)

```{r best4-2, cache = FALSE}
plot_tree(2)
table_probs(2)
```

## Best 4 (3/4)

```{r best4-3, cache = FALSE}
plot_tree(3)
table_probs(3)
```

## Best 4 (4/4)

```{r best4-4, cache = FALSE}
plot_tree(4)
table_probs(4)
```

## Worse 4 (1/4)

```{r worse4-1, cache = FALSE}
plot_tree(nrow(top_10_cases)/2 + 1)
table_probs(nrow(top_10_cases)/2 + 1)
```

## Worse 4 (2/4)

```{r worse4-2, cache = FALSE}
plot_tree(nrow(top_10_cases)/2 + 2)
table_probs(nrow(top_10_cases)/2 + 2)
```

## Worse 4 (3/4)

```{r worse4-3, cache = FALSE}
plot_tree(nrow(top_10_cases)/2 + 3)
table_probs(nrow(top_10_cases)/2 + 3)
```

## Worse 4 (4/4)

```{r  worse4-4, cache = FALSE}
plot_tree(nrow(top_10_cases)/2 + 4)
table_probs(nrow(top_10_cases)/2 + 4)
```

